================================================================================
                    ECHOSCAPE - AI REFLECTION & VISUAL COMPANION
                         COMPREHENSIVE ARCHITECTURE DOCUMENT
================================================================================

Document Version: 1.0
Last Updated: December 5, 2025
Author: Lead Architect

================================================================================
TABLE OF CONTENTS
================================================================================

1. EXECUTIVE SUMMARY
2. PRODUCT VISION & CORE FLOW
3. TECH STACK DECISIONS
4. SYSTEM ARCHITECTURE OVERVIEW
5. PHASE 1: BACKEND IMPLEMENTATION
6. PHASE 2: ML ANALYTICS SERVICE
7. PHASE 3: FIBO VISUAL SERVICE
8. PHASE 4: FRONTEND IMPLEMENTATION
9. PHASE 5: 3D VISUALIZATION (THREE.JS)
10. DATABASE SCHEMA
11. API SPECIFICATIONS
12. SAFETY & ETHICAL GUIDELINES
13. IMPLEMENTATION ROADMAP
14. TESTING STRATEGY

================================================================================
1. EXECUTIVE SUMMARY
================================================================================

EchoScape is an AI-powered emotional reflection and visual journaling companion.
It is NOT therapy - it's a structured self-reflection tool that:

  1. Guides users through conversational journaling sessions
  2. Analyzes emotional content using ML models
  3. Generates visual metaphors using FIBO (controllable text-to-image)
  4. Allows users to "reframe" their emotional landscape visually
  5. Tracks emotional journey in an interactive 3D space

Key Differentiators:
  - ML-driven emotion analysis (not just LLM API calls)
  - Controllable visual generation via FIBO's structured JSON
  - Interactive reframing as a visual CBT exercise
  - 3D emotional atlas for journey visualization

================================================================================
2. PRODUCT VISION & CORE FLOW
================================================================================

2.1 ONE SESSION FLOW (10-15 minutes)
------------------------------------

Step 1: User Starts Session
  - Opens app, clicks "New Reflection"
  - System creates session record in database
  - AI greets with: "What's on your mind today?"

Step 2: Conversational Journaling (5-10 turns)
  - User shares thoughts via text or voice
  - AI responds with reflective questions, NOT advice
  - Examples:
    * "Tell me more about how that made you feel..."
    * "When you say 'overwhelmed', what does that look like for you?"
    * "It sounds like you're carrying a lot right now."
  
  Implementation:
    - Each message saved to database
    - Real-time emotion analysis on user messages
    - Running emotion vector updated throughout session
    - Pattern detection for cognitive distortions

Step 3: Session Wrap-Up
  - AI signals natural ending: "We've covered a lot today..."
  - System generates:
    * Summary (2-3 sentences)
    * Emotion vector: {anxiety: 0.7, hope: 0.4, pride: 0.3}
    * Themes: ["perfectionism", "work-life balance"]
    * Metaphor: "walking through fog toward a clearing"

Step 4: Visual Generation (FIBO)
  - Emotion vector → Visual parameters mapping
  - Call FIBO API with structured JSON prompt
  - Display: "This is your emotional landscape for today"

Step 5: Interactive Reframing
  - Show sliders: Hope | Intensity | Openness
  - User adjusts → system modifies FIBO JSON
  - Regenerate with: "Here's a gentler perspective"
  - Save both original and reframed images

Step 6: Session Saved to History
  - Session becomes a "node" in 3D emotional atlas
  - Color-coded by primary emotion
  - Positioned in space based on emotion vector

================================================================================
3. TECH STACK DECISIONS
================================================================================

3.1 FRONTEND
------------
Framework:      Next.js 15 (App Router)
Language:       TypeScript
Styling:        Tailwind CSS + shadcn/ui
3D Graphics:    react-three-fiber + @react-three/drei
Animations:     Framer Motion
State:          Zustand (simple, performant)
Forms:          React Hook Form + Zod validation

WHY THESE CHOICES:
  - Next.js: SSR for SEO, API routes for BFF pattern, excellent DX
  - react-three-fiber: React-native 3D, no separate Three.js codebase
  - Zustand: Lighter than Redux, great for real-time state updates
  - shadcn/ui: Great defaults, fully customizable, not a component library

3.2 BACKEND
-----------
Primary:        Node.js with Express/Next.js API routes
Language:       TypeScript
ML Service:     Python FastAPI (separate microservice)
Database:       PostgreSQL via Supabase
Auth:           Supabase Auth (optional for MVP)
Queue:          BullMQ with Redis (for async image generation)

WHY THESE CHOICES:
  - TypeScript everywhere possible for type safety
  - Python FastAPI for ML service (best ML library ecosystem)
  - Supabase: Free tier, real-time, auth built-in, Postgres
  - BullMQ: Image generation can take 5-15s, needs async handling

3.3 AI & ML
-----------
LLM:            Google Gemini 1.5 Flash (fast, cost-effective)
Emotion Model:  Fine-tuned RoBERTa or GoEmotions model
Image Gen:      FIBO via fal.ai API
Embeddings:     (Future) For session similarity/clustering

WHY THESE CHOICES:
  - Gemini Flash: Good balance of speed/cost/quality
  - Separate emotion model: Shows ML depth, not just API wrapper
  - fal.ai: Hosted FIBO, fast inference, easy API

3.4 INFRASTRUCTURE
------------------
Hosting:        Vercel (frontend + API routes)
ML Service:     Railway or Render (Python container)
Database:       Supabase (managed Postgres)
Storage:        Supabase Storage or Cloudinary (images)
Monitoring:     Vercel Analytics + Sentry

================================================================================
4. SYSTEM ARCHITECTURE OVERVIEW
================================================================================

                    ┌─────────────────────────────────┐
                    │         FRONTEND (Next.js)      │
                    │  ┌───────────┐ ┌─────────────┐  │
                    │  │ Chat View │ │ 3D Galaxy   │  │
                    │  └─────┬─────┘ └──────┬──────┘  │
                    │        │              │         │
                    │  ┌─────┴──────────────┴─────┐   │
                    │  │     State (Zustand)      │   │
                    │  └────────────┬─────────────┘   │
                    └───────────────┼─────────────────┘
                                    │
                                    ▼
                    ┌─────────────────────────────────┐
                    │      API GATEWAY (Next.js)      │
                    │         /api/...                │
                    └───────┬─────────┬───────────────┘
                            │         │
              ┌─────────────┘         └─────────────┐
              ▼                                     ▼
┌─────────────────────────┐           ┌─────────────────────────┐
│  CONVERSATION SERVICE   │           │   ML ANALYTICS SERVICE  │
│  (Node.js / Next.js)    │◄─────────►│   (Python FastAPI)      │
│                         │           │                         │
│  - LLM orchestration    │           │  - Emotion classifier   │
│  - Message handling     │           │  - Pattern detector     │
│  - Summary generation   │           │  - Emotion aggregation  │
└───────────┬─────────────┘           └─────────────────────────┘
            │
            ▼
┌─────────────────────────┐           ┌─────────────────────────┐
│    FIBO VISUAL SERVICE  │           │      SUPABASE           │
│    (Node.js)            │           │                         │
│                         │           │  - PostgreSQL DB        │
│  - Emotion → JSON map   │           │  - Auth                 │
│  - FIBO API calls       │           │  - Storage (images)     │
│  - Image storage        │           │  - Real-time            │
└─────────────────────────┘           └─────────────────────────┘

================================================================================
5. PHASE 1: BACKEND IMPLEMENTATION
================================================================================

ALREADY COMPLETED:
  ✓ Database schema (sessions, crisis_logs)
  ✓ Supabase connection
  ✓ Type definitions (emotions, sessions, API)
  ✓ Services (gemini, emotion, fibo, crisis, session)
  ✓ Controllers (analyzer, generate, history)

REMAINING BACKEND TASKS:

5.1 CONVERSATION SERVICE
------------------------
Location: backend/src/services/conversation.service.ts

Purpose: Manage multi-turn chat sessions with LLM

Implementation Steps:

  a) Create conversation history manager
     - Store messages in DB as they're sent
     - Retrieve conversation history for context
     - Limit context window (last 20 messages)

  b) Design system prompt for AI coach
     ```
     You are a warm, reflective journaling companion. Your role is to:
     - Listen attentively and mirror emotions
     - Ask open-ended reflective questions
     - Help users explore their feelings
     - Avoid giving advice or diagnoses
     - Use empathetic, non-clinical language
     
     You are NOT a therapist. Never claim to be.
     If crisis language detected, provide resources.
     ```

  c) Implement turn-based conversation
     - User message → Add to context
     - Send to Gemini with system prompt
     - Get response → Save to DB
     - Return response to frontend

  d) Implement session ending detection
     - Natural conversation conclusion
     - User says "done" / "that's all"
     - Time limit (15 minutes)
     - Trigger summary generation

File: backend/src/services/conversation.service.ts
```typescript
interface ConversationContext {
  sessionId: string;
  messages: Message[];
  emotionVector: EmotionVector;
  startedAt: Date;
}

async function addUserMessage(sessionId: string, text: string): Promise<Message>
async function generateAIResponse(context: ConversationContext): Promise<string>
async function shouldEndSession(context: ConversationContext): boolean
async function generateSessionSummary(context: ConversationContext): Promise<SessionSummary>
```

5.2 MESSAGE PERSISTENCE
-----------------------
New Table: messages

CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant')),
  content TEXT NOT NULL,
  emotion_snapshot JSONB,  -- emotion at time of message
  created_at TIMESTAMPTZ DEFAULT NOW()
);

5.3 SUMMARY SERVICE
-------------------
Location: backend/src/services/summary.service.ts

Purpose: Generate end-of-session summary with emotions, themes, metaphor

Implementation:
  a) Collect all messages from session
  b) Send to Gemini with summary prompt
  c) Parse structured JSON response
  d) Validate emotion values (0-1 range)
  e) Store in session record

Summary Prompt:
```
Analyze this conversation and return ONLY valid JSON:
{
  "summary": "2-3 sentence summary of what was discussed",
  "emotions": {
    "anxiety": 0.0-1.0,
    "sadness": 0.0-1.0,
    "anger": 0.0-1.0,
    "fear": 0.0-1.0,
    "joy": 0.0-1.0,
    "hope": 0.0-1.0,
    "calm": 0.0-1.0
  },
  "themes": ["theme1", "theme2", "theme3"],
  "metaphor": "A vivid visual metaphor for their emotional state"
}
```

================================================================================
6. PHASE 2: ML ANALYTICS SERVICE
================================================================================

This is a separate Python microservice that provides real ML capabilities.

6.1 SERVICE SETUP
-----------------
Location: ml-service/ (new directory at project root)

Structure:
  ml-service/
  ├── app/
  │   ├── __init__.py
  │   ├── main.py              # FastAPI app
  │   ├── models/
  │   │   ├── emotion.py       # Emotion classifier
  │   │   └── patterns.py      # Cognitive pattern detector
  │   ├── services/
  │   │   ├── analyzer.py      # Analysis orchestration
  │   │   └── mapper.py        # Emotion to visual mapping
  │   └── routes/
  │       └── analyze.py       # API endpoints
  ├── requirements.txt
  ├── Dockerfile
  └── README.md

6.2 EMOTION CLASSIFIER
----------------------
File: ml-service/app/models/emotion.py

Option A: Use pre-trained model (faster setup)
  - HuggingFace: "bhadresh-savani/distilbert-base-uncased-emotion"
  - Or: "SamLowe/roberta-base-go_emotions"

Option B: Fine-tune on therapy/journaling data (better results)
  - Base: RoBERTa-base
  - Dataset: GoEmotions + custom labeled journal entries

Implementation:
```python
from transformers import pipeline

class EmotionClassifier:
    def __init__(self):
        self.classifier = pipeline(
            "text-classification",
            model="SamLowe/roberta-base-go_emotions",
            top_k=None
        )
    
    def predict(self, text: str) -> dict[str, float]:
        """Returns emotion vector like {joy: 0.3, sadness: 0.7, ...}"""
        results = self.classifier(text)
        return {r['label']: r['score'] for r in results[0]}
    
    def aggregate_session(self, messages: list[str]) -> dict[str, float]:
        """Aggregate emotions across all messages in session."""
        vectors = [self.predict(msg) for msg in messages]
        # Average vectors, weighted by recency
        ...
```

6.3 COGNITIVE PATTERN DETECTOR
------------------------------
File: ml-service/app/models/patterns.py

Detects cognitive distortions common in negative thinking:
  - All-or-nothing thinking: "I always fail", "nothing ever works"
  - Catastrophizing: "this will ruin everything"
  - Mind reading: "they must think I'm terrible"
  - Should statements: "I should have known better"
  - Personalization: "it's all my fault"

Implementation Options:

Option A: Rule-based + LLM fallback
```python
PATTERNS = {
    "all_or_nothing": ["always", "never", "nothing", "everything", "completely"],
    "catastrophizing": ["disaster", "ruined", "worst", "terrible", "end of"],
    "should_statements": ["should have", "must have", "ought to", "have to"],
}

def detect_patterns(text: str) -> list[str]:
    detected = []
    text_lower = text.lower()
    for pattern, keywords in PATTERNS.items():
        if any(kw in text_lower for kw in keywords):
            detected.append(pattern)
    return detected
```

Option B: Small fine-tuned classifier
  - Train on labeled cognitive distortion dataset
  - Multi-label classification
  - More accurate but requires training data

6.4 EMOTION TO VISUAL MAPPING
-----------------------------
File: ml-service/app/services/mapper.py

This is the key ML component that bridges emotions to FIBO parameters.

Visual Parameters to Map:
  - camera_distance: 0.0 (close, intimate) to 1.0 (wide, expansive)
  - light_intensity: 0.0 (dark) to 1.0 (bright)
  - color_temperature: 0.0 (cool blues) to 1.0 (warm oranges)
  - composition_openness: 0.0 (crowded) to 1.0 (spacious)
  - atmosphere_clarity: 0.0 (foggy) to 1.0 (clear)

Mapping Logic:

```python
def map_emotions_to_visuals(emotions: dict[str, float]) -> dict:
    # Extract key emotion dimensions
    valence = emotions.get('joy', 0) - emotions.get('sadness', 0)
    arousal = emotions.get('anxiety', 0) + emotions.get('anger', 0)
    hope = emotions.get('hope', 0)
    calm = emotions.get('calm', 0)
    
    return {
        "camera_distance": 0.5 + (valence * 0.3) + (hope * 0.2),
        "light_intensity": 0.3 + (valence * 0.4) + (hope * 0.3),
        "color_temperature": 0.5 + (valence * 0.3),
        "composition_openness": 0.4 + (calm * 0.3) + (hope * 0.3),
        "atmosphere_clarity": 0.5 + (calm * 0.3) - (arousal * 0.2),
    }
```

For a more ML approach, train a small neural network:
  - Input: 7-dimensional emotion vector
  - Output: 5-dimensional visual parameter vector
  - Architecture: 2-layer MLP
  - Training data: Manually labeled examples

6.5 API ENDPOINTS
-----------------
File: ml-service/app/routes/analyze.py

POST /analyze/emotion
  Input: { "text": "I feel so overwhelmed..." }
  Output: { "emotions": {...}, "patterns": [...] }

POST /analyze/session
  Input: { "messages": ["msg1", "msg2", ...] }
  Output: { "aggregated_emotions": {...}, "all_patterns": [...] }

POST /analyze/visual-params
  Input: { "emotions": {...} }
  Output: { "camera_distance": 0.7, "light_intensity": 0.5, ... }

================================================================================
7. PHASE 3: FIBO VISUAL SERVICE
================================================================================

7.1 UNDERSTANDING FIBO
----------------------
FIBO is a controllable text-to-image model that understands structured JSON.
It lets you specify camera, lighting, composition, style separately.

FIBO JSON Schema (simplified):
```json
{
  "prompt": "A serene mountain lake at dawn",
  "camera": {
    "angle": "wide",
    "distance": "far",
    "height": "eye-level"
  },
  "lighting": {
    "time_of_day": "dawn",
    "intensity": 0.7,
    "direction": "side"
  },
  "composition": {
    "focus": "landscape",
    "space": "open",
    "tension": "low"
  },
  "style": {
    "aesthetic": "minimalist",
    "palette": "cool pastels",
    "mood": "peaceful"
  }
}
```

7.2 IMPLEMENTATION
------------------
File: backend/src/services/fibo.service.ts (UPDATE)

```typescript
interface FiboRequest {
  prompt: string;
  camera: {
    angle: 'close' | 'medium' | 'wide' | 'panoramic';
    distance: number;  // 0-1
    height: 'low' | 'eye-level' | 'high' | 'birds-eye';
  };
  lighting: {
    time_of_day: string;
    intensity: number;  // 0-1
    direction: 'front' | 'side' | 'back' | 'ambient';
  };
  composition: {
    openness: number;  // 0-1
    tension: number;   // 0-1
  };
  style: {
    aesthetic: string;
    palette: string;
    mood: string;
  };
}

async function buildFiboPrompt(
  metaphor: string,
  visualParams: VisualParams
): FiboRequest {
  return {
    prompt: metaphor,
    camera: {
      angle: mapDistanceToAngle(visualParams.camera_distance),
      distance: visualParams.camera_distance,
      height: 'eye-level'
    },
    lighting: {
      time_of_day: visualParams.light_intensity > 0.6 ? 'day' : 'dusk',
      intensity: visualParams.light_intensity,
      direction: 'side'
    },
    composition: {
      openness: visualParams.composition_openness,
      tension: 1 - visualParams.atmosphere_clarity
    },
    style: {
      aesthetic: 'minimalist illustration',
      palette: mapTemperatureToPalette(visualParams.color_temperature),
      mood: deriveMood(visualParams)
    }
  };
}

async function generateWithFibo(request: FiboRequest): Promise<string> {
  // Call fal.ai FIBO API
  const response = await fetch('https://fal.run/fal-ai/fibo', {
    method: 'POST',
    headers: {
      'Authorization': `Key ${process.env.FAL_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(request)
  });
  
  const result = await response.json();
  return result.images[0].url;
}
```

7.3 REFRAMING FEATURE
---------------------
When user adjusts sliders (hope, intensity, openness):

```typescript
async function reframeImage(
  sessionId: string,
  adjustments: {
    hopeDelta: number;      // -0.5 to 0.5
    intensityDelta: number; // -0.5 to 0.5
    opennessDelta: number;  // -0.5 to 0.5
  }
): Promise<string> {
  // Get original visual params
  const session = await getSession(sessionId);
  const originalParams = session.visual_params;
  
  // Apply adjustments
  const adjustedParams = {
    ...originalParams,
    light_intensity: clamp(originalParams.light_intensity + adjustments.hopeDelta * 0.4),
    composition_openness: clamp(originalParams.composition_openness + adjustments.opennessDelta * 0.3),
    atmosphere_clarity: clamp(originalParams.atmosphere_clarity + adjustments.hopeDelta * 0.2),
  };
  
  // Regenerate with adjusted params
  const fiboRequest = buildFiboPrompt(session.metaphor, adjustedParams);
  const newImageUrl = await generateWithFibo(fiboRequest);
  
  // Save reframed image
  await updateSession(sessionId, {
    reframed_image_url: newImageUrl,
    reframe_params: adjustments
  });
  
  return newImageUrl;
}
```

================================================================================
8. PHASE 4: FRONTEND IMPLEMENTATION
================================================================================

8.1 PROJECT STRUCTURE
---------------------
Location: frontend/ (or use existing Next.js app)

Structure:
  frontend/
  ├── app/
  │   ├── layout.tsx
  │   ├── page.tsx                 # Landing/home
  │   ├── session/
  │   │   ├── page.tsx             # New session
  │   │   └── [id]/page.tsx        # Session detail
  │   ├── history/
  │   │   └── page.tsx             # 3D galaxy view
  │   └── api/                     # API routes (BFF)
  ├── components/
  │   ├── chat/
  │   │   ├── ChatPanel.tsx
  │   │   ├── MessageBubble.tsx
  │   │   └── InputBar.tsx
  │   ├── visual/
  │   │   ├── EmotionImage.tsx
  │   │   ├── ReframeSliders.tsx
  │   │   └── EmotionTags.tsx
  │   ├── three/
  │   │   ├── Galaxy.tsx
  │   │   ├── SessionNode.tsx
  │   │   └── CameraControls.tsx
  │   └── ui/                      # shadcn components
  ├── lib/
  │   ├── api.ts                   # API client
  │   ├── store.ts                 # Zustand store
  │   └── utils.ts
  └── styles/
      └── globals.css

8.2 SESSION VIEW IMPLEMENTATION
-------------------------------
File: frontend/app/session/page.tsx

Layout:
  ┌──────────────────────────────────────────────────────────┐
  │  HEADER: EchoScape                            [End]      │
  ├────────────────────────┬─────────────────────────────────┤
  │                        │                                 │
  │   CHAT PANEL           │   VISUAL PANEL                  │
  │                        │                                 │
  │   ┌──────────────────┐ │   ┌───────────────────────────┐ │
  │   │ AI: What's on    │ │   │                           │ │
  │   │ your mind today? │ │   │    [FIBO Image]           │ │
  │   └──────────────────┘ │   │                           │ │
  │                        │   │                           │ │
  │   ┌──────────────────┐ │   └───────────────────────────┘ │
  │   │ User: I've been  │ │                                 │
  │   │ feeling really...│ │   Hope:      ─────●───────      │
  │   └──────────────────┘ │   Intensity: ───────●─────      │
  │                        │   Openness:  ─────────●───      │
  │   [   Type here...  ] │                                 │
  │                        │   Tags: anxious • hopeful       │
  └────────────────────────┴─────────────────────────────────┘

Component Structure:
```tsx
// app/session/page.tsx
export default function SessionPage() {
  return (
    <div className="flex h-screen">
      <ChatPanel className="w-1/2 border-r" />
      <VisualPanel className="w-1/2" />
    </div>
  );
}

// components/chat/ChatPanel.tsx
export function ChatPanel() {
  const { messages, sendMessage, isLoading } = useChat();
  
  return (
    <div className="flex flex-col h-full">
      <div className="flex-1 overflow-y-auto p-4">
        {messages.map(msg => (
          <MessageBubble key={msg.id} message={msg} />
        ))}
      </div>
      <InputBar onSend={sendMessage} disabled={isLoading} />
    </div>
  );
}

// components/visual/VisualPanel.tsx
export function VisualPanel() {
  const { image, emotions, sliders, updateSliders } = useSession();
  
  return (
    <div className="flex flex-col h-full p-4">
      <EmotionImage src={image} className="flex-1" />
      <ReframeSliders values={sliders} onChange={updateSliders} />
      <EmotionTags emotions={emotions} />
    </div>
  );
}
```

8.3 STATE MANAGEMENT
--------------------
File: frontend/lib/store.ts

```typescript
import { create } from 'zustand';

interface SessionState {
  sessionId: string | null;
  messages: Message[];
  emotions: EmotionVector;
  visualParams: VisualParams;
  imageUrl: string | null;
  reframedImageUrl: string | null;
  sliders: { hope: number; intensity: number; openness: number };
  isLoading: boolean;
  
  // Actions
  startSession: () => Promise<void>;
  sendMessage: (text: string) => Promise<void>;
  updateSliders: (sliders: Partial<typeof this.sliders>) => void;
  reframeImage: () => Promise<void>;
  endSession: () => Promise<void>;
}

export const useSessionStore = create<SessionState>((set, get) => ({
  // ... implementation
}));
```

================================================================================
9. PHASE 5: 3D VISUALIZATION (THREE.JS)
================================================================================

9.1 CONCEPT: EMOTIONAL GALAXY
-----------------------------
Each session becomes a "planet" or "orb" in 3D space.
Position determined by emotion vector (using dimensionality reduction).
Color determined by primary emotion.
Size determined by session intensity.

9.2 IMPLEMENTATION
------------------
File: frontend/components/three/Galaxy.tsx

```tsx
import { Canvas } from '@react-three/fiber';
import { OrbitControls, Stars } from '@react-three/drei';

export function Galaxy({ sessions }: { sessions: Session[] }) {
  return (
    <Canvas camera={{ position: [0, 0, 50] }}>
      <ambientLight intensity={0.5} />
      <pointLight position={[10, 10, 10]} />
      
      <Stars radius={100} depth={50} count={5000} fade />
      
      {sessions.map(session => (
        <SessionNode 
          key={session.id}
          session={session}
          position={calculatePosition(session.emotions)}
        />
      ))}
      
      <OrbitControls enableZoom enablePan enableRotate />
    </Canvas>
  );
}
```

File: frontend/components/three/SessionNode.tsx

```tsx
import { useRef, useState } from 'react';
import { useFrame } from '@react-three/fiber';
import { Sphere, Html } from '@react-three/drei';

export function SessionNode({ session, position }) {
  const meshRef = useRef();
  const [hovered, setHovered] = useState(false);
  
  const color = emotionToColor(session.emotions);
  const size = 0.5 + session.intensity * 0.5;
  
  useFrame(() => {
    meshRef.current.rotation.y += 0.01;
  });
  
  return (
    <group position={position}>
      <Sphere
        ref={meshRef}
        args={[size, 32, 32]}
        onPointerOver={() => setHovered(true)}
        onPointerOut={() => setHovered(false)}
        onClick={() => navigateToSession(session.id)}
      >
        <meshStandardMaterial 
          color={color}
          emissive={color}
          emissiveIntensity={hovered ? 0.5 : 0.2}
        />
      </Sphere>
      
      {hovered && (
        <Html distanceFactor={10}>
          <div className="bg-black/80 text-white p-2 rounded text-sm">
            {new Date(session.created_at).toLocaleDateString()}
          </div>
        </Html>
      )}
    </group>
  );
}

function emotionToColor(emotions: EmotionVector): string {
  // Map emotions to colors
  if (emotions.joy > 0.5) return '#FFD700';  // Gold
  if (emotions.sadness > 0.5) return '#4169E1';  // Blue
  if (emotions.anxiety > 0.5) return '#9370DB';  // Purple
  if (emotions.anger > 0.5) return '#DC143C';  // Red
  if (emotions.calm > 0.5) return '#20B2AA';  // Teal
  return '#808080';  // Gray default
}

function calculatePosition(emotions: EmotionVector): [number, number, number] {
  // Simple mapping: use PCA or t-SNE for better results
  const x = (emotions.joy - emotions.sadness) * 20;
  const y = (emotions.hope - emotions.anxiety) * 20;
  const z = (emotions.calm - emotions.anger) * 20;
  return [x, y, z];
}
```

================================================================================
10. DATABASE SCHEMA
================================================================================

-- Enable UUID
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Users (optional for MVP)
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email TEXT UNIQUE,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Sessions
CREATE TABLE sessions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  ended_at TIMESTAMPTZ,
  
  -- Content
  journal_text TEXT,  -- Full transcript
  summary TEXT,
  themes TEXT[],
  metaphor TEXT,
  
  -- Emotion data
  emotion_data JSONB NOT NULL,
  primary_emotion TEXT,
  
  -- Visuals
  visual_params JSONB NOT NULL,
  original_image_url TEXT,
  reframed_image_url TEXT,
  reframe_params JSONB,
  
  -- Safety
  crisis_detected BOOLEAN DEFAULT FALSE
);

-- Messages (for conversation history)
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT NOT NULL,
  emotion_snapshot JSONB,
  patterns_detected TEXT[],
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Crisis logs (for safety auditing)
CREATE TABLE crisis_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID REFERENCES sessions(id) ON DELETE SET NULL,
  trigger_text TEXT NOT NULL,
  risk_score DECIMAL(3,2),
  detected_keywords TEXT[],
  resources_shown BOOLEAN DEFAULT TRUE,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Analytics (for ML insights)
CREATE TABLE session_analytics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
  emotion_vector JSONB,
  cognitive_patterns TEXT[],
  visual_params_used JSONB,
  ml_model_version TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_sessions_user ON sessions(user_id);
CREATE INDEX idx_sessions_created ON sessions(created_at DESC);
CREATE INDEX idx_messages_session ON messages(session_id);
CREATE INDEX idx_crisis_session ON crisis_logs(session_id);

================================================================================
11. API SPECIFICATIONS
================================================================================

BASE URL: /api

11.1 SESSION ENDPOINTS
----------------------

POST /api/session/start
  Description: Start a new reflection session
  Request: { user_id?: string }
  Response: { 
    success: true, 
    data: { session_id: string, greeting: string } 
  }

POST /api/session/:id/message
  Description: Send a message in the session
  Request: { role: "user", content: string }
  Response: { 
    success: true,
    data: {
      message: Message,
      ai_response: Message,
      emotion_update: EmotionVector
    }
  }

POST /api/session/:id/end
  Description: End session and generate summary
  Request: {}
  Response: {
    success: true,
    data: {
      summary: string,
      themes: string[],
      emotions: EmotionVector,
      metaphor: string,
      image_url: string
    }
  }

GET /api/session/:id
  Description: Get full session details
  Response: { success: true, data: Session }

11.2 VISUAL ENDPOINTS
---------------------

POST /api/visual/generate
  Description: Generate FIBO image from emotion data
  Request: {
    session_id: string,
    emotions: EmotionVector,
    metaphor: string
  }
  Response: { success: true, data: { image_url: string } }

POST /api/visual/reframe
  Description: Adjust and regenerate image
  Request: {
    session_id: string,
    adjustments: {
      hope_delta: number,
      intensity_delta: number,
      openness_delta: number
    }
  }
  Response: { success: true, data: { image_url: string } }

11.3 HISTORY ENDPOINTS
----------------------

GET /api/history
  Description: Get all sessions for 3D view
  Query: ?page=1&limit=50
  Response: {
    success: true,
    data: Session[],
    pagination: { ... }
  }

11.4 ML SERVICE ENDPOINTS
-------------------------

POST /ml/analyze/emotion
  Description: Classify emotion from text
  Request: { text: string }
  Response: { emotions: EmotionVector, patterns: string[] }

POST /ml/analyze/session
  Description: Analyze full session
  Request: { messages: string[] }
  Response: { 
    aggregated_emotions: EmotionVector,
    all_patterns: string[],
    visual_params: VisualParams
  }

================================================================================
12. SAFETY & ETHICAL GUIDELINES
================================================================================

12.1 REQUIRED DISCLAIMERS
-------------------------
Display prominently on:
  - Landing page
  - Session start
  - About page

Text:
  "EchoScape is a self-reflection and journaling tool.
   It is NOT a replacement for professional mental health care.
   If you are in crisis, please contact emergency services or:
   - National Suicide Prevention Lifeline: 988
   - Crisis Text Line: Text HOME to 741741"

12.2 CRISIS DETECTION
---------------------
Implemented in: backend/src/services/crisis.service.ts

When crisis detected:
  1. AI does NOT continue normal coaching
  2. Show immediate crisis resources
  3. Do NOT generate intense/dark imagery
  4. Log event (without PII) for safety auditing

12.3 CONTENT GUIDELINES FOR AI
------------------------------
System prompt must include:
  - "Never diagnose or claim to treat conditions"
  - "Never use clinical terminology"
  - "Always suggest professional help for serious concerns"
  - "Focus on feelings, not fixing"

12.4 DATA PRIVACY
-----------------
  - All data encrypted at rest (Supabase default)
  - Sessions can be deleted by user
  - No selling or sharing of data
  - HIPAA not required (not medical device) but follow best practices

================================================================================
13. IMPLEMENTATION ROADMAP
================================================================================

WEEK 1: FOUNDATION (Days 1-7)
-----------------------------
Day 1-2: 
  [ ] Set up Next.js frontend with Tailwind
  [ ] Set up Python ML service with FastAPI
  [ ] Configure Supabase with updated schema
  
Day 3-4:
  [ ] Implement conversation service
  [ ] Implement message storage
  [ ] Basic chat UI component
  
Day 5-6:
  [ ] Integrate Gemini for AI responses
  [ ] Implement emotion classifier in ML service
  [ ] Connect frontend to backend
  
Day 7:
  [ ] End-to-end chat flow working
  [ ] Basic emotion display
  [ ] Code review and fixes

WEEK 2: VISUALS (Days 8-14)
---------------------------
Day 8-9:
  [ ] Implement summary generation
  [ ] Emotion to visual params mapping
  [ ] FIBO API integration
  
Day 10-11:
  [ ] Build visual panel UI
  [ ] Implement reframe sliders
  [ ] Connect sliders to FIBO regeneration
  
Day 12-13:
  [ ] Polish session flow
  [ ] Add emotion tags and themes
  [ ] Image loading states and animations
  
Day 14:
  [ ] Full session flow working
  [ ] Testing and bug fixes

WEEK 3: 3D & POLISH (Days 15-21)
--------------------------------
Day 15-16:
  [ ] Set up Three.js galaxy view
  [ ] Implement session nodes
  [ ] Camera controls and navigation
  
Day 17-18:
  [ ] Connect galaxy to real session data
  [ ] Add session detail popup
  [ ] Styling and polish
  
Day 19-20:
  [ ] Crisis detection and safety
  [ ] Error handling throughout
  [ ] Loading states and animations
  
Day 21:
  [ ] Final testing
  [ ] Bug fixes
  [ ] Documentation

HACKATHON SCOPE (48-72 hours)
-----------------------------
If time-constrained, prioritize:
  [MUST] Chat interface with LLM
  [MUST] Basic emotion extraction
  [MUST] FIBO image generation
  [SHOULD] Reframe sliders
  [SHOULD] Session history list
  [NICE] 3D galaxy (can be 2D fallback)

================================================================================
14. TESTING STRATEGY
================================================================================

14.1 UNIT TESTS
---------------
Backend:
  - Emotion service: mapping logic
  - Crisis service: keyword detection
  - Session service: CRUD operations

ML Service:
  - Emotion classifier accuracy
  - Pattern detection coverage
  - Visual params mapping ranges

14.2 INTEGRATION TESTS
----------------------
  - Chat flow: message → AI response → emotion update
  - Visual flow: session end → summary → image generation
  - Reframe flow: slider change → new image

14.3 E2E TESTS
--------------
Using Playwright:
  - Complete session from start to image
  - Reframe an existing session
  - View session history
  - Navigate 3D galaxy

14.4 MANUAL TESTING
-------------------
  - Crisis detection scenarios
  - Edge cases (empty messages, very long text)
  - Mobile responsiveness
  - 3D performance on various devices

================================================================================
END OF DOCUMENT
================================================================================

This document serves as the complete technical blueprint for EchoScape.
Each section can be implemented independently by following the specifications.

For questions or clarifications, refer to the relevant section number.
